{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "AOC Stats Rainclouds — Combined (Sternberg + N-back)\n",
    "\n",
    "- Load merged data\n",
    "- Remove outliers (1.5×IQR rule)\n",
    "- Compute descriptive statistics\n",
    "- Perform within-subject repeated-measures ANOVAs\n",
    "- Fit linear mixed models (subject random intercepts) -> export as word table\n",
    "- Compute pairwise condition contrasts (Bonferroni-adjusted) and within-subject effect sizes (Cohen’s dz + 95% CI)\n",
    "- Create raincloud-style visualisations (half-violin + jittered dots + box) for each variable and task\n",
    "- Adds significance brackets across conditions\n",
    "- Saves all figures and statistical output tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from functions.stats_helpers import (\n",
    "    iqr_outlier_filter,\n",
    "    mixedlm_pairwise_contrasts,\n",
    "    p_to_signif\n",
    ")\n",
    "\n",
    "from functions.rainclouds_plotting_helpers import add_stat_brackets\n",
    "\n",
    "from functions.export_model_table import export_model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Colours\n",
    "pal = [\"#93B8C4\", \"#82AD82\", \"#D998A2\"]  # AOC pastels\n",
    "\n",
    "# Plot appearance\n",
    "mpl.rcParams.update({\n",
    "    \"figure.dpi\": 160,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"savefig.transparent\": False,\n",
    "    \"savefig.facecolor\": \"white\",\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"ps.fonttype\": 42,\n",
    "    \"font.size\": 15,\n",
    "    \"axes.titlesize\": 15,\n",
    "    \"axes.labelsize\": 15,\n",
    "    \"legend.fontsize\": 15,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"mathtext.default\": \"regular\",\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"figure.edgecolor\": \"white\",\n",
    "    \"axes.edgecolor\": \"white\",\n",
    "})\n",
    "\n",
    "sns.set_style(\"white\")  # clean white background, no grey panel\n",
    "\n",
    "# %% I/O Paths\n",
    "base_dir   = \"/Volumes/g_psyplafor_methlab$/Students/Arne/AOC\"\n",
    "output_dir = f\"{base_dir}/figures/stats/rainclouds\"\n",
    "output_dir_stats = f\"{base_dir}/data/stats\"\n",
    "anova_dir = f\"{base_dir}/data/stats/anova\"\n",
    "\n",
    "# %% Variables and labelling\n",
    "\n",
    "variables  = [\"Accuracy\", \"ReactionTime\", \"GazeDeviation\", \"MSRate\", \"Fixations\", \"Saccades\", \"ScanPathLength\", \"AlphaPower\", \"IAF\"]\n",
    "titles     = [\"Accuracy\", \"Reaction Time\", \"Gaze Deviation\", \"Microsaccade Rate\", \"Fixations\", \"Saccades\", \"Scan Path Length\", \"Alpha Power\", \"IAF\"]\n",
    "y_labels   = [\"Accuracy [%]\", \"Reaction Time [s]\", \"Gaze Deviation [px]\", \"Microsaccade Rate [MS/s]\", \"Fixations\", \"Saccades\", \"Scan Path Length [px]\", \"Alpha Power [\\u03BCV²/Hz]\", \"IAF [Hz]\"]\n",
    "save_names = [\"acc\", \"rt\", \"gazedev\", \"ms\", \"fix\", \"sacc\", \"spl\", \"pow\", \"iaf\"]\n",
    "\n",
    "# Manual y ticks and ylims per variable\n",
    "yticks_map = {\n",
    "    \"Accuracy\"      : np.arange(60, 101, 5),\n",
    "    \"ReactionTime\"  : np.arange(0.3, 1.35, 0.1),\n",
    "    \"GazeDeviation\" : np.arange(0, 65, 10),\n",
    "    \"MSRate\"        : np.arange(0, 4.1, 0.5),\n",
    "    \"Fixations\"     : np.arange(0, 8.5, 1),\n",
    "    \"Saccades\"      : np.arange(0, 4.25, 1),\n",
    "    \"ScanPathLength\": np.arange(0, 410, 50),\n",
    "    \"AlphaPower\"    : np.arange(0, 1.52, 0.25),\n",
    "    \"IAF\"           : np.arange(8, 14, 1),\n",
    "}\n",
    "ylims_map = {\n",
    "    \"Accuracy\"      : (60, 102),\n",
    "    \"ReactionTime\"  : (0.3, 1.35),\n",
    "    \"GazeDeviation\" : (0, 65),\n",
    "    \"MSRate\"        : (0, 4.1),\n",
    "    \"Fixations\"     : (0, 8.5),\n",
    "    \"Saccades\"      : (0, 4.25),\n",
    "    \"ScanPathLength\": (0, 410),\n",
    "    \"AlphaPower\"    : (0, 1.6),\n",
    "    \"IAF\"           : (8, 14),\n",
    "}\n",
    "\n",
    "# %% Task configurations\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        \"name\"       : \"sternberg\",\n",
    "        \"input_csv\"  : f\"{base_dir}/data/features/merged_data_sternberg.csv\",\n",
    "        # Accept numeric encodings {1,2,3} or {2,4,6}; otherwise normalise existing strings\n",
    "        \"cond_to_label_numeric\": [{1: \"WM load 2\", 2: \"WM load 4\", 3: \"WM load 6\"},\n",
    "                                  {2: \"WM load 2\", 4: \"WM load 4\", 6: \"WM load 6\"}],\n",
    "        \"categories\" : [\"WM load 2\", \"WM load 4\", \"WM load 6\"],\n",
    "        \"comparisons\": [(\"WM load 2\", \"WM load 4\"), (\"WM load 2\", \"WM load 6\"), (\"WM load 4\", \"WM load 6\")],\n",
    "        \"xlabel\"     : \"Condition\"\n",
    "    },\n",
    "    {\n",
    "        \"name\"       : \"nback\",\n",
    "        \"input_csv\"  : f\"{base_dir}/data/features/merged_data_nback.csv\",\n",
    "        \"cond_to_label_numeric\": [{1: \"1-back\", 2: \"2-back\", 3: \"3-back\"}],\n",
    "        \"categories\" : [\"1-back\", \"2-back\", \"3-back\"],\n",
    "        \"comparisons\": [(\"1-back\", \"2-back\"), (\"1-back\", \"3-back\"), (\"2-back\", \"3-back\")],\n",
    "        \"xlabel\"     : \"Condition\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# %% Pre-scan for global upper bounds (shared bracket baseline per variable)\n",
    "global_upper = {var: np.nan for var in variables}\n",
    "\n",
    "for _task in tasks:\n",
    "    _dat = pd.read_csv(_task[\"input_csv\"])\n",
    "    _dat.loc[_dat[\"Accuracy\"] > 100, \"Accuracy\"] = np.nan  # same impossible-value rule\n",
    "\n",
    "    # normalise/label Condition exactly like in the main loop\n",
    "    _cond = _dat[\"Condition\"]\n",
    "    if np.issubdtype(_cond.dtype, np.number):\n",
    "        _uniq = sorted(pd.unique(_cond.dropna()).tolist())\n",
    "        _applied_map = None\n",
    "        for _cand in _task[\"cond_to_label_numeric\"]:\n",
    "            if set(_uniq).issubset(set(_cand.keys())):\n",
    "                _applied_map = _cand\n",
    "                break\n",
    "        if _applied_map is None:\n",
    "            _applied_map = {val: _task[\"categories\"][i] for i, val in enumerate(_uniq[:len(_task[\"categories\"])])}\n",
    "        _dat[\"Condition\"] = _dat[\"Condition\"].map(_applied_map)\n",
    "    else:\n",
    "        _dat[\"Condition\"] = _dat[\"Condition\"].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    _dat[\"Condition\"] = pd.Categorical(_dat[\"Condition\"], categories=_task[\"categories\"], ordered=True)\n",
    "    if _dat[\"ID\"].dtype != \"O\":\n",
    "        _dat[\"ID\"] = _dat[\"ID\"].astype(str)\n",
    "\n",
    "    # apply the same outlier filter as the main pipeline\n",
    "    _dat = iqr_outlier_filter(_dat, variables, by=\"Condition\")\n",
    "\n",
    "    # update global upper bounds\n",
    "    for var in variables:\n",
    "        vmax = pd.to_numeric(_dat[var], errors=\"coerce\").max()\n",
    "        if np.isfinite(vmax):\n",
    "            if not np.isfinite(global_upper[var]) or (vmax > global_upper[var]):\n",
    "                global_upper[var] = float(vmax)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing + plotting\n",
    "\n",
    "for task in tasks:\n",
    "\n",
    "    # %% Load\n",
    "    dat = pd.read_csv(task[\"input_csv\"])\n",
    "\n",
    "    # Remove impossible values\n",
    "    dat.loc[dat[\"Accuracy\"] > 100, \"Accuracy\"] = np.nan\n",
    "\n",
    "    # %% Condition labelling (robust to numeric or already-labelled strings)\n",
    "    cond = dat[\"Condition\"]\n",
    "    if np.issubdtype(cond.dtype, np.number):\n",
    "        uniq = sorted(pd.unique(cond.dropna()).tolist())\n",
    "        applied_map = None\n",
    "        for candidate_map in task[\"cond_to_label_numeric\"]:\n",
    "            if set(uniq).issubset(set(candidate_map.keys())):\n",
    "                applied_map = candidate_map\n",
    "                break\n",
    "        if applied_map is None:\n",
    "            # Fallback by rank order\n",
    "            applied_map = {val: task[\"categories\"][i] for i, val in enumerate(uniq[:len(task[\"categories\"])])}\n",
    "        dat[\"Condition\"] = dat[\"Condition\"].map(applied_map)\n",
    "    else:\n",
    "        dat[\"Condition\"] = dat[\"Condition\"].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    dat[\"Condition\"] = pd.Categorical(dat[\"Condition\"], categories=task[\"categories\"], ordered=True)\n",
    "\n",
    "    # Ensure ID is string for grouping\n",
    "    if dat[\"ID\"].dtype != \"O\":\n",
    "        dat[\"ID\"] = dat[\"ID\"].astype(str)\n",
    "\n",
    "    # Outlier removal per condition & variable (1.5×IQR)\n",
    "    dat = iqr_outlier_filter(dat, variables, by=\"Condition\")\n",
    "\n",
    "    # %% Descriptive statistics per condition (save as CSV)\n",
    "    desc_rows = []\n",
    "\n",
    "    for var in variables:\n",
    "        if var not in dat.columns:\n",
    "            continue\n",
    "\n",
    "        subdat = dat.loc[~dat[var].isna(), [\"ID\", \"Condition\", var]].copy()\n",
    "        if subdat.empty:\n",
    "            continue\n",
    "\n",
    "        # Group by Condition (subjects already collapsed to one row per condition)\n",
    "        for cond_lab, g in subdat.groupby(\"Condition\", observed=True):\n",
    "            if g.empty:\n",
    "                continue\n",
    "\n",
    "            vals = g[var].to_numpy()\n",
    "\n",
    "            n     = vals.size\n",
    "            mean  = np.nanmean(vals)\n",
    "            sd    = np.nanstd(vals, ddof=1)\n",
    "            sem   = sd / np.sqrt(n) if n > 0 else np.nan\n",
    "            med   = np.nanmedian(vals)\n",
    "            q1    = np.nanpercentile(vals, 25)\n",
    "            q3    = np.nanpercentile(vals, 75)\n",
    "            iqr   = q3 - q1\n",
    "\n",
    "            desc_rows.append([\n",
    "                task[\"name\"], var, cond_lab, n,\n",
    "                round(mean, 2), round(sd, 2), round(sem, 2),\n",
    "                round(med, 2), round(q1, 2), round(q3, 2), round(iqr, 2)\n",
    "            ])\n",
    "\n",
    "    desc_table = pd.DataFrame(\n",
    "        desc_rows,\n",
    "        columns=[\n",
    "            \"Task\", \"Variable\", \"Condition\", \"N\",\n",
    "            \"Mean\", \"SD\", \"SEM\",\n",
    "            \"Median\", \"Q1\", \"Q3\", \"IQR\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Save descriptive summary for the task\n",
    "    out_csv = os.path.join(output_dir_stats, f\"AOC_descriptives_{task['name']}.csv\")\n",
    "    desc_table.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved descriptives → {out_csv}\")\n",
    "\n",
    "    # Category order / palette mapping\n",
    "    condition_order = list(dat[\"Condition\"].dropna().unique())\n",
    "    pal_dict = dict(zip(condition_order, pal))\n",
    "\n",
    "    anova_rows = []               # collects ANOVA rows for this task\n",
    "    pairwise_effsize_rows = []    # collects pairwise effect sizes for this task\n",
    "\n",
    "    # %% Loop variables\n",
    "    for var, ttl, ylab, sname in zip(variables, titles, y_labels, save_names):\n",
    "\n",
    "        dvar = dat.loc[~dat[var].isna(), [\"ID\", \"Condition\", var]].copy()\n",
    "        if dvar.empty:\n",
    "            continue\n",
    "\n",
    "        # Ensure categorical ordering\n",
    "        dvar[\"Condition\"] = pd.Categorical(dvar[\"Condition\"], categories=condition_order, ordered=True)\n",
    "\n",
    "        # %% Repeated-measures ANOVA (within-subject: Condition)\n",
    "        # balance subjects: keep only subjects present in all conditions for this var\n",
    "        present = dvar.groupby('ID')['Condition'].nunique()\n",
    "        keep_ids = present[present == len(condition_order)].index\n",
    "        dvar_bal = dvar[dvar['ID'].isin(keep_ids)].copy()\n",
    "\n",
    "        # run ANOVA on the balanced panel\n",
    "        if dvar_bal['ID'].nunique() >= 2:\n",
    "            aov = AnovaRM(data=dvar_bal, depvar=var, subject='ID', within=['Condition']).fit()\n",
    "            aov_tab = aov.anova_table.reset_index().rename(columns={'index': 'Effect'})\n",
    "            for _, r in aov_tab.iterrows():\n",
    "                F   = float(r['F Value'])\n",
    "                df1 = float(r['Num DF'])\n",
    "                df2 = float(r['Den DF'])\n",
    "                p   = float(r['Pr > F'])\n",
    "                etap = (F * df1) / (F * df1 + df2) if np.isfinite(F) else np.nan\n",
    "                anova_rows.append([task['name'], var, r['Effect'], df1, df2, F, p, etap])\n",
    "        else:\n",
    "            anova_rows.append([task['name'], var, 'Condition', np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "        # Data bounds (used to size violins and brackets)\n",
    "        lower_bound = float(dvar[var].min())\n",
    "        upper_bound = float(dvar[var].max())\n",
    "\n",
    "        # Mixed model + Bonferroni pairwise\n",
    "        pw = mixedlm_pairwise_contrasts(\n",
    "            dvar.rename(columns={var: \"value\"}),\n",
    "            value_col=\"value\",\n",
    "            group_col=\"Condition\",\n",
    "            id_col=\"ID\",\n",
    "            p_adjust=\"bonferroni\"\n",
    "        )\n",
    "\n",
    "        # %% Pairwise within-subject effect sizes (Cohen's dz) + 95% CI of mean difference\n",
    "        # build wide table to get paired diffs\n",
    "        wide = dvar.pivot(index='ID', columns='Condition', values=var)\n",
    "        for (g1, g2) in task[\"comparisons\"]:\n",
    "            if (g1 in wide.columns) and (g2 in wide.columns):\n",
    "                diffs = (wide[g2] - wide[g1]).dropna()\n",
    "                n = diffs.shape[0]\n",
    "                if n >= 2 and np.nanstd(diffs, ddof=1) > 0:\n",
    "                    md   = float(np.nanmean(diffs))\n",
    "                    sd_d = float(np.nanstd(diffs, ddof=1))\n",
    "                    dz   = md / sd_d\n",
    "                    # 95% CI for mean difference (paired t): md ± t*sd_d/sqrt(n)\n",
    "                    from scipy import stats\n",
    "                    tcrit = stats.t.ppf(0.975, df=n-1)\n",
    "                    se_md = sd_d / np.sqrt(n)\n",
    "                    ci_lo = md - tcrit * se_md\n",
    "                    ci_hi = md + tcrit * se_md\n",
    "                else:\n",
    "                    md = dz = ci_lo = ci_hi = np.nan\n",
    "                    n = int(n)\n",
    "                # attach adjusted p if available\n",
    "                padj = np.nan\n",
    "                row = pw.loc[(pw[\"group1\"] == g1) & (pw[\"group2\"] == g2)]\n",
    "                if not row.empty and \"p_adj\" in row:\n",
    "                    try:\n",
    "                        padj = float(row[\"p_adj\"].iloc[0])\n",
    "                    except Exception:\n",
    "                        padj = np.nan\n",
    "                pairwise_effsize_rows.append([\n",
    "                    task[\"name\"], var, g1, g2, n, md, dz, ci_lo, ci_hi, padj\n",
    "                ])\n",
    "\n",
    "        # %% Fit mixed model to export as a Word table\n",
    "        # Random-intercept model (subjects), Condition as fixed effect.\n",
    "        import statsmodels.formula.api as smf\n",
    "\n",
    "        dvar_m = dvar.rename(columns={var: \"value\"}).copy()\n",
    "        # Ensure Condition is categorical with your order (already set above)\n",
    "        dvar_m[\"Condition\"] = pd.Categorical(dvar_m[\"Condition\"],\n",
    "                                            categories=condition_order, ordered=True)\n",
    "\n",
    "        # Use treatment coding with the FIRST level as reference (your ordered categories)\n",
    "        # reml=False -> so the reported log-likelihood is comparable across models.\n",
    "        # If it struggles to converge, it will fall back to random-intercepts only and try a different optimizer.\n",
    "        model_result = None\n",
    "        try:\n",
    "            # Random intercepts only\n",
    "            m = smf.mixedlm(\"value ~ C(Condition, Treatment(reference=condition_order[0]))\",\n",
    "                            data=dvar_m, groups=dvar_m[\"ID\"])\n",
    "            model_result = m.fit(method=\"lbfgs\", reml=False, maxiter=200, disp=False)\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                # Try Nelder-Mead as a fallback\n",
    "                model_result = m.fit(method=\"nm\", reml=False, maxiter=400, disp=False)\n",
    "            except Exception as e2:\n",
    "                # Final fallback: simple OLS (no random effects), so you still get a table\n",
    "                m_ols = smf.ols(\"value ~ C(Condition, Treatment(reference=condition_order[0]))\",\n",
    "                                data=dvar_m).fit()\n",
    "                model_result = m_ols\n",
    "\n",
    "        # Export to Word\n",
    "        doc_name = f\"AOC_modeltable_{sname}_{task['name']}.docx\"\n",
    "        doc_path = os.path.join(output_dir_stats, doc_name)\n",
    "        export_model_table(model_result, doc_path)\n",
    "        print(f\"Saved model table → {doc_path}\")\n",
    "\n",
    "        # %% Figure\n",
    "        fig, ax = plt.subplots(figsize=(8, 6), facecolor=\"white\")\n",
    "        fig.patch.set_alpha(1.0)\n",
    "        ax.patch.set_alpha(1.0)\n",
    "        ax.set_facecolor(\"white\")\n",
    "\n",
    "        # %% Manual raincloud parameters\n",
    "        viol_alpha  = 0.60\n",
    "        dot_alpha   = 0.50\n",
    "        dot_size    = 30\n",
    "        box_width   = 0.20\n",
    "        cloud_offset = -0.20\n",
    "        max_violsw  = 0.40\n",
    "        bw_method   = 0.15\n",
    "        if var == \"Accuracy\":\n",
    "            cloud_offset = -0.20\n",
    "            max_violsw   = 0.50\n",
    "            bw_method    = 0.25\n",
    "\n",
    "        # x positions for categories\n",
    "        xpos = {c: i for i, c in enumerate(condition_order)}\n",
    "\n",
    "        # Deterministic jitter\n",
    "        rng = np.random.default_rng(12345)\n",
    "\n",
    "        # %% Draw per condition\n",
    "        for cond_lab in condition_order:\n",
    "            yvals = dvar.loc[dvar[\"Condition\"] == cond_lab, var].dropna().to_numpy()\n",
    "            if yvals.size == 0:\n",
    "                continue\n",
    "\n",
    "            # VIOLIN (left half)\n",
    "            # Determine hard cap for this variable\n",
    "            ymax_cap = ylims_map[var][1] if var in ylims_map else float(dvar[var].max())\n",
    "            ymin_cap = ylims_map[var][0] if var in ylims_map else float(dvar[var].min())\n",
    "            yr = ymax_cap - ymin_cap\n",
    "\n",
    "            pad_top = min(0.02 * yr, 0.3)\n",
    "            y_grid_top = ymin_cap + yr + pad_top\n",
    "\n",
    "            # build KDE\n",
    "            kde = gaussian_kde(yvals, bw_method=bw_method)  # <-- REINSERT THIS LINE\n",
    "\n",
    "            # grid\n",
    "            y_grid = np.linspace(ymin_cap, y_grid_top, 400)\n",
    "\n",
    "            dens = kde(y_grid)\n",
    "            scale = (max_violsw / np.nanmax(dens)) if np.nanmax(dens) > 0 else 0.0\n",
    "\n",
    "            x_left  = xpos[cond_lab] + cloud_offset - dens * scale\n",
    "            x_right = np.full_like(y_grid, xpos[cond_lab] + cloud_offset)\n",
    "\n",
    "            poly_x = np.concatenate([x_right, x_left[::-1]])\n",
    "            poly_y = np.concatenate([y_grid, y_grid[::-1]])\n",
    "            ax.fill(poly_x, poly_y,\n",
    "                    facecolor=pal_dict[cond_lab], edgecolor=\"none\",\n",
    "                    alpha=viol_alpha, clip_on=True)\n",
    "\n",
    "            # DOTS\n",
    "            x_jit = xpos[cond_lab] + rng.uniform(-box_width / 2, box_width / 2, size=yvals.size)\n",
    "            ax.scatter(x_jit, yvals, s=dot_size, alpha=dot_alpha, color=pal_dict[cond_lab], linewidths=0, zorder=3)\n",
    "\n",
    "            # BOXPLOT\n",
    "            bp = ax.boxplot(\n",
    "                [yvals], positions=[xpos[cond_lab]], widths=box_width, vert=True,\n",
    "                patch_artist=True, showfliers=False, whis=(5, 95),\n",
    "                medianprops=dict(color=\"black\", linewidth=1.5),\n",
    "                boxprops=dict(linewidth=1.0, edgecolor=\"black\"),\n",
    "                whiskerprops=dict(linewidth=1.0, color=\"black\"),\n",
    "                capprops=dict(linewidth=1.0, color=\"black\"),\n",
    "                meanline=False, showmeans=False\n",
    "            )\n",
    "            for patch in bp[\"boxes\"]:\n",
    "                patch.set_facecolor(mpl.colors.to_rgba(pal_dict[cond_lab], 0.05))\n",
    "                patch.set_edgecolor(\"black\")\n",
    "            for elem in [\"whiskers\", \"caps\", \"medians\"]:\n",
    "                for artist in bp[elem]:\n",
    "                    artist.set_color(\"black\")\n",
    "\n",
    "        # No legend\n",
    "        leg = ax.get_legend()\n",
    "        if leg is not None:\n",
    "            leg.remove()\n",
    "\n",
    "        # Clean spines and grid\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        ax.yaxis.grid(True, linewidth=1, alpha=0.35)\n",
    "        ax.xaxis.grid(False)\n",
    "\n",
    "        # Labels and x-ticks\n",
    "        ax.set_title(\"\")\n",
    "        ax.set_xticks(range(len(condition_order)))\n",
    "        ax.set_xticklabels(condition_order)\n",
    "        ax.set_xlabel(\"\")\n",
    "        if len(condition_order) >= 2:\n",
    "            ax.annotate(\n",
    "                task[\"xlabel\"],\n",
    "                xy=(xpos[condition_order[1]], 0),\n",
    "                xycoords=(\"data\", \"axes fraction\"),\n",
    "                xytext=(0, -28),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\", va=\"top\"\n",
    "            )\n",
    "\n",
    "        # %% Bracket layout with shared (global) ymax per variable\n",
    "        ymin = float(dvar[var].min()) if np.isfinite(dvar[var].min()) else 0.0\n",
    "        ymax_data_local = float(dvar[var].max()) if np.isfinite(dvar[var].max()) else ymin\n",
    "        ymax_cap = global_upper.get(var, np.nan)\n",
    "        if not np.isfinite(ymax_cap):\n",
    "            ymax_cap = ymax_data_local  # fallback if pre-scan found nothing\n",
    "\n",
    "        # use the global cap for bracket baseline and ylim ceiling\n",
    "        range_y = max(ymax_cap - ymin, 1.0)\n",
    "        head = 0.02 * range_y\n",
    "        step = 0.10 * range_y\n",
    "\n",
    "        y_positions = []\n",
    "        start = ymax_cap + 0.075 * range_y\n",
    "        for i in range(len(task[\"comparisons\"])):\n",
    "            y_positions.append(start + i * step)\n",
    "\n",
    "        # y-label at data midpoint\n",
    "        ymin_cur, ymax_cur = ylims_map[var]\n",
    "        ymid = (ymin_cur + ymax_cap) / 2.0\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.yaxis.get_label().set_visible(False)\n",
    "        ax.text(\n",
    "            -0.12,\n",
    "            ymid,\n",
    "            ylab,\n",
    "            transform=ax.get_yaxis_transform(which='grid'),\n",
    "            rotation=90,\n",
    "            ha='center',\n",
    "            va='center'\n",
    "        )\n",
    "\n",
    "        # Signif labels\n",
    "        labels = []\n",
    "        for (g1, g2) in task[\"comparisons\"]:\n",
    "            row = pw.loc[(pw[\"group1\"] == g1) & (pw[\"group2\"] == g2)]\n",
    "            labels.append(\"n.s.\" if row.empty else p_to_signif(float(row[\"p_adj\"].iloc[0])))\n",
    "\n",
    "        # %% slightly increase bracket spacing only for Accuracy in N-back\n",
    "        if (task[\"name\"] == \"nback\") and (var == \"Accuracy\"):\n",
    "            yr = ax.get_ylim()[1] - ax.get_ylim()[0]\n",
    "            # assume you already computed y_positions; just spread them a bit more\n",
    "            step_bump = 0.025 * yr   # +2.5% of axis range on each successive bracket\n",
    "            y_positions = [y + i * step_bump for i, y in enumerate(y_positions)]\n",
    "\n",
    "        add_stat_brackets(\n",
    "            ax=ax,\n",
    "            xcats=condition_order,\n",
    "            comparisons=task[\"comparisons\"],\n",
    "            y_positions=y_positions,\n",
    "            labels=labels,\n",
    "            xmap=xpos\n",
    "        )\n",
    "\n",
    "        # %% Manual y-ticks (identical for both tasks)\n",
    "        if var in yticks_map:\n",
    "            ax.set_yticks(yticks_map[var])\n",
    "\n",
    "        if var in ylims_map:\n",
    "            ymin_set, ymax_set = ylims_map[var]\n",
    "            ax.set_ylim(ymin_set, ymax_set)\n",
    "\n",
    "        # %% Save raincloud figure for each variable\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(\n",
    "            os.path.join(output_dir, f\"AOC_stats_rainclouds_{sname}_{task['name']}_stats.png\"),\n",
    "            dpi=300,\n",
    "            transparent=False,\n",
    "            facecolor=fig.get_facecolor(),\n",
    "            edgecolor=fig.get_edgecolor() if hasattr(fig, \"get_edgecolor\") else \"white\"\n",
    "        )\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Save ANOVA for this task\n",
    "    anova_df = pd.DataFrame(\n",
    "        anova_rows,\n",
    "        columns=[\"Task\", \"Variable\", \"Effect\", \"DF_num\", \"DF_den\", \"F\", \"p\", \"eta_p2\"]\n",
    "    )\n",
    "    anova_csv = os.path.join(anova_dir, f\"AOC_anova_{task['name']}.csv\")\n",
    "    anova_df.to_csv(anova_csv, index=False)\n",
    "    print(f\"Saved ANOVA → {anova_csv}\")\n",
    "\n",
    "    # Save pairwise effect sizes for this task\n",
    "    pw_eff_df = pd.DataFrame(\n",
    "        pairwise_effsize_rows,\n",
    "        columns=[\"Task\", \"Variable\", \"Group1\", \"Group2\", \"N\", \"MeanDiff\", \"Cohens_dz\", \"CI95_low\", \"CI95_high\", \"p_adj\"]\n",
    "    )\n",
    "    pw_csv = os.path.join(output_dir_stats, f\"AOC_pairwise_effectsizes_{task['name']}.csv\")\n",
    "    pw_eff_df.to_csv(pw_csv, index=False)\n",
    "    print(f\"Saved pairwise effect sizes → {pw_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoc-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
